sage maker Steps for Integration with databricks

1. IAM & Networking Setup

Attach IAM role to Databricks cluster with permissions:

{
  "Effect": "Allow",
  "Action": [
    "sagemaker:*",
    "s3:*",
    "logs:*",
    "cloudwatch:*"
  ],
  "Resource": "*"
}


Ensure Databricks & SageMaker use the same VPC and S3 buckets for seamless data transfer.

2. Install Required Libraries in Databricks

Run in a notebook cell:

%pip install boto3 sagemaker

3. Preprocess Data in Databricks
# Example: Write processed data to S3 for SageMaker training
df = spark.read.format("delta").load("s3://my-databricks-delta/dataset")
df = df.toPandas()

# Save to S3
import boto3
s3 = boto3.client("s3")
df.to_csv("/tmp/train.csv", index=False)
s3.upload_file("/tmp/train.csv", "my-sagemaker-bucket", "train/train.csv")

4. Launch SageMaker Training from Databricks
import sagemaker
from sagemaker import get_execution_role
from sagemaker.sklearn.estimator import SKLearn

session = sagemaker.Session()

sklearn_estimator = SKLearn(
    entry_point="train.py",           # training script
    role=get_execution_role(),
    instance_type="ml.m5.large",
    framework_version="1.0-1",
    py_version="py3",
    sagemaker_session=session
)

sklearn_estimator.fit({"train": "s3://my-sagemaker-bucket/train/"})

5. Deploy Model as Endpoint
predictor = sklearn_estimator.deploy(
    initial_instance_count=1,
    instance_type="ml.m5.large"
)

6. Call SageMaker Endpoint from Databricks
response = predictor.predict([[5.1, 3.5, 1.4, 0.2]])
print(response)